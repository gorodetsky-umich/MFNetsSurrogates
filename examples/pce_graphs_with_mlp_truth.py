"""
An advanced example comparing different PCE-based model architectures against
a "true" process generated by a neural network.

This script performs the following steps:
1. Defines a "true" 3-fidelity data-generating process using MLPs in a
   hierarchical graph structure (1 -> 2 -> 3).
2. Generates training and testing data from this true process.
3. Defines two candidate graph architectures: Peer and Hierarchical.
4. For each architecture, trains models using 1st, 2nd, 3rd, and 4th degree
   Polynomial Chaos Expansions (PCEs).
5. Creates a single, comprehensive 2x5 plot that visualizes each graph
   structure and the prediction performance for each PCE degree.
"""
import os
from functools import partial

import jax
import jax.numpy as jnp
import networkx as nx
import optax
from jax import tree_util
from matplotlib import pyplot as plt

from mfnets_surrogates import (
    MFNetJax,
    MLPModel,
    PCEnhancementModel,
    init_mlp_params,
    init_mlp_enhancement_model,
    init_pce_model,
    init_pc_enhancement_model,
    mse_loss_graph,
)

# --- Plotting and Graph Helpers ---


def plot_graph_on_ax(ax, graph: nx.DiGraph, title: str):
    """Draw a NetworkX graph on a given Matplotlib Axes object."""
    pos = nx.spring_layout(graph, seed=42)
    nx.draw(
        graph,
        pos,
        ax=ax,
        with_labels=True,
        node_size=2000,
        node_color="#a2d2ff",
        font_size=14,
        font_weight="bold",
        arrowsize=20,
    )
    ax.set_title(title, fontsize=16)


def plot_predictions_on_ax(ax, y_true, y_pred, mse: float, title: str):
    """Draw a predicted vs. actual scatter plot on a given Axes object."""
    ax.scatter(y_true, y_pred, alpha=0.6)
    ax.plot(
        [y_true.min(), y_true.max()],
        [y_true.min(), y_true.max()],
        "r--",
        lw=2,
    )
    ax.text(
        0.95,
        0.05,
        f"Test MSE: {mse:.4f}",
        verticalalignment="bottom",
        horizontalalignment="right",
        transform=ax.transAxes,
        fontsize=12,
        bbox={"boxstyle": "round,pad=0.5", "facecolor": "wheat", "alpha": 0.5},
    )
    ax.set_xlabel("True Values", fontsize=10)
    ax.set_ylabel("Predicted Values", fontsize=10)
    ax.set_title(title, fontsize=14)
    ax.grid(True, linestyle="--", alpha=0.6)
    ax.set_aspect("equal", adjustable="box")


def train_graph(mfnet: MFNetJax, x_train, y_train, num_steps=15000):
    """A helper function to run the Optax training loop for a given graph."""
    target_nodes = tuple(sorted(mfnet.graph.nodes))
    params, treedef = tree_util.tree_flatten(mfnet)
    # Use a lower learning rate for stability with higher-order polynomials
    optimizer = optax.adam(learning_rate=5e-3)
    opt_state = optimizer.init(params)

    loss_fn = partial(_calculate_loss, treedef=treedef, target_nodes=target_nodes)

    @jax.jit
    def step(p, opt_s, x, y):
        grads = jax.grad(loss_fn)(p, x, y)
        updates, opt_s = optimizer.update(grads, opt_s)
        p = optax.apply_updates(p, updates)
        return p, opt_s

    for i in range(num_steps):
        params, opt_state = step(params, opt_state, x_train, y_train)

    return treedef.unflatten(params)


def _calculate_loss(current_params, x, y, treedef, target_nodes):
    """Helper for loss calculation, compatible with jax.grad."""
    model = treedef.unflatten(current_params)
    return mse_loss_graph(model, target_nodes, x, y)


# --- Main Experiment ---


def main():
    """Run the PCE graph comparison experiment."""
    key = jax.random.PRNGKey(0)
    d_in, d_out = 1, 1
    os.makedirs("plots", exist_ok=True)

    # 1. Define a "True" Model using MLPs in a hierarchical structure
    print("--- 1. Generating data from a 'true' MLP-based model ---")
    key, m1_key, m2_key, m3_key = jax.random.split(key, 4)
    true_m1 = MLPModel(init_mlp_params(m1_key, [d_in, 16, d_out]), jax.nn.tanh)
    true_m2 = init_mlp_enhancement_model(
        m2_key, [d_in + d_out, 16, d_out], jax.nn.tanh
    )
    true_m3 = init_mlp_enhancement_model(
        m3_key, [d_in + d_out, 16, d_out], jax.nn.tanh
    )
    true_graph = nx.DiGraph([(1, 2), (2, 3)])
    true_graph.add_node(1, func=true_m1)
    true_graph.add_node(2, func=true_m2)
    true_graph.add_node(3, func=true_m3)
    true_mfnet = MFNetJax(true_graph)

    # Generate training and testing data splits
    x_all = jnp.linspace(-2, 2, 400).reshape(-1, d_in)
    y1_all, y2_all, y3_all = true_mfnet.run((1, 2, 3), x_all)
    y_all = (y1_all, y2_all, y3_all)

    key, subkey = jax.random.split(key)
    train_indices = jax.random.permutation(subkey, 400)[:200]
    test_indices = jnp.setdiff1d(jnp.arange(400), train_indices)
    x_train, x_test = x_all[train_indices], x_all[test_indices]
    y_train = tuple(y[train_indices] for y in y_all)
    y_test = tuple(y[test_indices] for y in y_all)
    y_true_hf = y_test[2]

    # --- Setup for the combined 2x5 plot (horizontal layout) ---
    fig, axes = plt.subplots(2, 5, figsize=(28, 11), dpi=120)
    fig.suptitle(
        "PCE Model Comparison (True Architecture: Hierarchical)", fontsize=24
    )

    graph_structures = {
        "Peer": (nx.DiGraph([(1, 3), (2, 3)]), axes[0, :]),
        "Hierarchical": (nx.DiGraph([(1, 2), (2, 3)]), axes[1, :]),
    }

    # 2. Loop through experiments
    for name, (graph_struct, ax_row) in graph_structures.items():
        print(f"\n--- 2. Training {name} Models ---")
        # First column is for the graph structure plot
        plot_graph_on_ax(ax_row[0], graph_struct, f"{name} Graph Structure")

        # --- UPDATED: Loop includes degree 4 ---
        for degree in [1, 2, 3, 4]:
            print(f"  Training with PCE Degree: {degree}...")
            key, pce_key = jax.random.split(key)
            key1, key2, key3 = jax.random.split(pce_key, 3)

            if name == "Peer":
                m1 = init_pce_model(key1, d_in, d_out, degree)
                m2 = init_pce_model(key2, d_in, d_out, degree)
                m3 = init_pc_enhancement_model(
                    key3, d_in, d_out * 2, d_out, degree
                )
                nodes = [(1, m1), (2, m2), (3, m3)]
            else:  # Hierarchical
                m1 = init_pce_model(key1, d_in, d_out, degree)
                m2 = init_pc_enhancement_model(key2, d_in, d_out, d_out, degree)
                m3 = init_pc_enhancement_model(key3, d_in, d_out, d_out, degree)
                nodes = [(1, m1), (2, m2), (3, m3)]

            # Create and train the graph
            graph = nx.DiGraph(graph_struct.edges)
            for node_id, model_func in nodes:
                graph.add_node(node_id, func=model_func)
            mfnet = MFNetJax(graph)

            mfnet_trained = train_graph(mfnet, x_train, y_train)

            # Evaluate and plot in the correct column
            (y_pred,) = mfnet_trained.run((3,), x_test)
            test_mse = jnp.mean((y_true_hf - y_pred) ** 2)
            # Plot starts from the second column (index 1)
            plot_predictions_on_ax(
                ax_row[degree], y_true_hf, y_pred, test_mse, f"PCE Degree {degree}"
            )

    # --- Finalize and Save the Combined Plot ---
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    save_path = "plots/pce_comparison.png"
    fig.savefig(save_path)
    plt.close(fig)

    print(f"\nExperiment complete. Check '{save_path}' for results.")


if __name__ == "__main__":
    main()

