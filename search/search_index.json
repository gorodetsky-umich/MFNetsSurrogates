{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MFNets-Surrogates","text":"<p>MFNets-Surrogates is a JAX-native library for building, training, and analyzing multi-fidelity surrogate models using flexible, differentiable graph structures.</p> <p>This documentation provides a comprehensive guide to the library, including a tutorial on how to get started and a full API reference.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>JAX Core: The entire library is built with JAX, utilizing <code>jax.jit</code> for compilation and <code>jax.grad</code> for automatic differentiation.</li> <li>End-to-End Differentiable: The <code>MFNetJax</code> class is registered as a JAX PyTree, making the graph structure transparent to JAX's transformations.</li> <li>Flexible Graph Structures: Uses NetworkX to define arbitrary directed acyclic graphs.</li> <li>Composable Models: Includes a suite of built-in models like <code>LinearModel</code>, <code>MLPModel</code>, and <code>PCEModel</code>.</li> </ul> <p>To get started, check out the Tutorial.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page contains the auto-generated API reference for the <code>mfnets_surrogates</code> package, generated directly from the docstrings.</p>"},{"location":"api/#mfnets_surrogates","title":"<code>mfnets_surrogates</code>","text":"<p>A JAX-based library for multi-fidelity surrogate modeling.</p>"},{"location":"api/#mfnets_surrogates.LinearModel","title":"<code>LinearModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>A simple linear model: y = x @ W.T + b.</p>"},{"location":"api/#mfnets_surrogates.LinearModel.__init__","title":"<code>__init__(params)</code>","text":"<p>Initialize the model with its parameters.</p>"},{"location":"api/#mfnets_surrogates.LinearModel.run","title":"<code>run(xin)</code>","text":"<p>Evaluate the model on a batch of input data.</p>"},{"location":"api/#mfnets_surrogates.LinearModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.LinearModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.LinearModel2D","title":"<code>LinearModel2D</code>","text":"<p>               Bases: <code>Model</code></p> <p>Linear model with a 2D matrix output for scaling matrices.</p>"},{"location":"api/#mfnets_surrogates.LinearModel2D.__init__","title":"<code>__init__(params)</code>","text":"<p>Initialize the model with its parameters.</p>"},{"location":"api/#mfnets_surrogates.LinearModel2D.run","title":"<code>run(xin)</code>","text":"<p>Evaluate the model on a batch of input data.</p>"},{"location":"api/#mfnets_surrogates.LinearModel2D.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.LinearModel2D.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.LinearParams","title":"<code>LinearParams</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Parameters for a linear model.</p>"},{"location":"api/#mfnets_surrogates.LinearScaleShiftModel","title":"<code>LinearScaleShiftModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>A model that computes a scale-and-shift correction.</p>"},{"location":"api/#mfnets_surrogates.LinearScaleShiftModel.__init__","title":"<code>__init__(edge_model, node_model)</code>","text":"<p>Initialize the model with its edge and node sub-models.</p>"},{"location":"api/#mfnets_surrogates.LinearScaleShiftModel.run","title":"<code>run(xin, parent_val)</code>","text":"<p>Evaluate the model: y = scale(x) @ parent_val + shift(x).</p>"},{"location":"api/#mfnets_surrogates.LinearScaleShiftModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.LinearScaleShiftModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.MFNetJax","title":"<code>MFNetJax</code>","text":"<p>A JAX-compatible multi-fidelity network represented by a directed graph.</p> <p>This class wraps a <code>networkx.DiGraph</code> where each node contains a callable \"func\" that represents a surrogate model. It is registered as a JAX PyTree, allowing its parameters to be transparently handled by JAX transformations like <code>jax.grad</code> and <code>jax.jit</code>.</p>"},{"location":"api/#mfnets_surrogates.MFNetJax--attributes","title":"Attributes","text":"<pre><code>graph (nx.DiGraph): The graphical representation of the MF network.\neval_order (list): A topologically sorted list of nodes for execution.\nparents (dict): A mapping from each node to its direct predecessors.\nancestors (dict): A mapping from each node to all its ancestors.\n</code></pre>"},{"location":"api/#mfnets_surrogates.MFNetJax.__init__","title":"<code>__init__(graph)</code>","text":"<p>Initialize the multifidelity network.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>A networkx.DiGraph where each node's data dictionary must    contain a \"func\" key pointing to a JAX-compatible model.</p> required"},{"location":"api/#mfnets_surrogates.MFNetJax.run","title":"<code>run(target_nodes, xinput)</code>","text":"<p>Evaluate the graph for the specified target nodes.</p>"},{"location":"api/#mfnets_surrogates.MFNetJax.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the MFNetJax into its dynamic leaves and static data.</p> <p>This method is required for JAX PyTree registration.</p>"},{"location":"api/#mfnets_surrogates.MFNetJax.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Reconstruct an MFNetJax from static data and dynamic leaves.</p> <p>This method is required for JAX PyTree registration.</p>"},{"location":"api/#mfnets_surrogates.MLPEnhancementModel","title":"<code>MLPEnhancementModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>An MLP that enhances a low-fidelity input with a high-fidelity one.</p>"},{"location":"api/#mfnets_surrogates.MLPEnhancementModel.__init__","title":"<code>__init__(mlp_model)</code>","text":"<p>Initialize the model with its internal MLP.</p>"},{"location":"api/#mfnets_surrogates.MLPEnhancementModel.run","title":"<code>run(xin, parent_val)</code>","text":"<p>Evaluate the model on a batch of inputs and parent values.</p>"},{"location":"api/#mfnets_surrogates.MLPEnhancementModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.MLPEnhancementModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.MLPModel","title":"<code>MLPModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>A Multi-Layer Perceptron (MLP) model.</p>"},{"location":"api/#mfnets_surrogates.MLPModel.__init__","title":"<code>__init__(params, activation=jnn.relu)</code>","text":"<p>Initialize the MLP with its parameters and activation function.</p>"},{"location":"api/#mfnets_surrogates.MLPModel.run","title":"<code>run(xin)</code>","text":"<p>Evaluate the MLP on a batch of input data.</p>"},{"location":"api/#mfnets_surrogates.MLPModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model into its parameters and static data.</p>"},{"location":"api/#mfnets_surrogates.MLPModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten the model from its parameters and static data.</p>"},{"location":"api/#mfnets_surrogates.Model","title":"<code>Model</code>","text":"<p>Base class for all models to ensure they are registered as PyTrees.</p>"},{"location":"api/#mfnets_surrogates.Model.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.Model.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.PCEAdditiveModel","title":"<code>PCEAdditiveModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>An additive enhancement model using PCE and a linear model.</p>"},{"location":"api/#mfnets_surrogates.PCEAdditiveModel.__init__","title":"<code>__init__(edge_model, node_model)</code>","text":"<p>Initialize the model with its edge and node sub-models.</p>"},{"location":"api/#mfnets_surrogates.PCEAdditiveModel.run","title":"<code>run(xin, parent_val)</code>","text":"<p>Evaluate the model on a batch of inputs and parent values.</p>"},{"location":"api/#mfnets_surrogates.PCEAdditiveModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.PCEAdditiveModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.PCEModel","title":"<code>PCEModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>A Polynomial Chaos Expansion model that outputs a vector.</p>"},{"location":"api/#mfnets_surrogates.PCEModel.__init__","title":"<code>__init__(params, poly_type, degree, multi_indices)</code>","text":"<p>Initialize the PCE model.</p>"},{"location":"api/#mfnets_surrogates.PCEModel.run","title":"<code>run(xin)</code>","text":"<p>Evaluate the PCE model on a batch of inputs.</p>"},{"location":"api/#mfnets_surrogates.PCEModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model into dynamic parameters and static data.</p>"},{"location":"api/#mfnets_surrogates.PCEModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten the model from its parameters and static data.</p>"},{"location":"api/#mfnets_surrogates.PCEModel2D","title":"<code>PCEModel2D</code>","text":"<p>               Bases: <code>Model</code></p> <p>A Polynomial Chaos Expansion model that outputs a matrix.</p>"},{"location":"api/#mfnets_surrogates.PCEModel2D.__init__","title":"<code>__init__(params, poly_type, degree, multi_indices)</code>","text":"<p>Initialize the PCE model.</p>"},{"location":"api/#mfnets_surrogates.PCEModel2D.run","title":"<code>run(xin)</code>","text":"<p>Evaluate the PCE model on a batch of inputs.</p>"},{"location":"api/#mfnets_surrogates.PCEModel2D.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model into dynamic parameters and static data.</p>"},{"location":"api/#mfnets_surrogates.PCEModel2D.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten the model from its parameters and static data.</p>"},{"location":"api/#mfnets_surrogates.PCEScaleShiftModel","title":"<code>PCEScaleShiftModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>An enhancement model using PCEs for both scale and shift terms.</p>"},{"location":"api/#mfnets_surrogates.PCEScaleShiftModel.__init__","title":"<code>__init__(edge_model, node_model)</code>","text":"<p>Initialize the model with its edge and node sub-models.</p>"},{"location":"api/#mfnets_surrogates.PCEScaleShiftModel.run","title":"<code>run(xin, parent_val)</code>","text":"<p>Evaluate the model: y = PCE_edge(x) @ parent_val + PCE_node(x).</p>"},{"location":"api/#mfnets_surrogates.PCEScaleShiftModel.tree_flatten","title":"<code>tree_flatten()</code>","text":"<p>Flatten the model's parameters into a list of arrays (leaves).</p>"},{"location":"api/#mfnets_surrogates.PCEScaleShiftModel.tree_unflatten","title":"<code>tree_unflatten(aux_data, children)</code>  <code>classmethod</code>","text":"<p>Unflatten parameter arrays back into a model instance.</p>"},{"location":"api/#mfnets_surrogates.build_poly_basis","title":"<code>build_poly_basis(x, multi_indices, poly_type, degree)</code>","text":"<p>Construct the PCE basis matrix for a batch of inputs.</p>"},{"location":"api/#mfnets_surrogates.init_linear2d_params","title":"<code>init_linear2d_params(key, d_out1, d_out2, d_in)</code>","text":"<p>Initialize parameters for a LinearModel2D.</p>"},{"location":"api/#mfnets_surrogates.init_linear_params","title":"<code>init_linear_params(key, dim_in, dim_out)</code>","text":"<p>Initialize parameters for a LinearModel.</p>"},{"location":"api/#mfnets_surrogates.init_linear_scale_shift_model","title":"<code>init_linear_scale_shift_model(key, d_in, d_parent, d_out)</code>","text":"<p>Initialize a complete LinearScaleShiftModel.</p>"},{"location":"api/#mfnets_surrogates.init_mlp_enhancement_model","title":"<code>init_mlp_enhancement_model(key, layer_sizes, activation=jnn.relu)</code>","text":"<p>Initialize a complete MLPEnhancementModel.</p>"},{"location":"api/#mfnets_surrogates.init_mlp_params","title":"<code>init_mlp_params(key, layer_sizes)</code>","text":"<p>Initialize all parameters for an MLP.</p>"},{"location":"api/#mfnets_surrogates.init_pc_additive_model","title":"<code>init_pc_additive_model(key, d_in, d_parent, d_out, degree, poly_type='hermite')</code>","text":"<p>Initialize a PCEAdditiveModel.</p>"},{"location":"api/#mfnets_surrogates.init_pce_model","title":"<code>init_pce_model(key, dim_in, dim_out, degree, poly_type='hermite')</code>","text":"<p>Initialize a PCEModel.</p>"},{"location":"api/#mfnets_surrogates.init_pce_model_2d","title":"<code>init_pce_model_2d(key, d_in, d_out1, d_out2, degree, poly_type='hermite')</code>","text":"<p>Initialize a PCEModel that outputs a 2D matrix.</p>"},{"location":"api/#mfnets_surrogates.init_pce_scale_shift_model","title":"<code>init_pce_scale_shift_model(key, d_in, d_parent, d_out, degree, poly_type='hermite')</code>","text":"<p>Initialize a PCEScaleShiftModel.</p>"},{"location":"api/#mfnets_surrogates.make_graph_2gen","title":"<code>make_graph_2gen(mod1, mod2)</code>","text":"<p>Create a simple two-node graph: 1 -&gt; 2.</p>"},{"location":"api/#mfnets_surrogates.mse_loss_graph","title":"<code>mse_loss_graph(model, nodes, x, y)</code>","text":"<p>Calculate the total mean squared error across multiple graph nodes.</p>"},{"location":"api/#mfnets_surrogates.resid_loss_graph","title":"<code>resid_loss_graph(model, nodes, x, y)</code>","text":"<p>Calculate the flattened residual vector for least-squares solvers.</p>"},{"location":"tutorial/","title":"Tutorial: Building Your First MFNet","text":"<p>This tutorial walks through a complete example of defining, training, and evaluating a simple two-fidelity hierarchical model (<code>1 -&gt; 2</code>).</p> <p>This is the same example found in the project's <code>README.md</code>.</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport networkx as nx\nimport optax\nfrom jax import tree_util\n\nfrom mfnets_surrogates import (\n    MFNetJax,\n    LinearModel,\n    LinearScaleShiftModel,\n    init_linear_params,\n    init_linear_scale_shift_model,\n    mse_loss_graph,\n)\n\ndef main():\n    \"\"\"A complete example of building, training, and running an MFNet.\"\"\"\n    key = jax.random.PRNGKey(0)\n    d_in, d1_out, d2_out = 2, 2, 3\n    key, true_key, train_key, data_key = jax.random.split(key, 4)\n\n    # 1. Define a \"true\" model and generate some training data\n    true_m1 = LinearModel(init_linear_params(true_key, d_in, d1_out))\n    true_m2 = init_linear_scale_shift_model(true_key, d_in, d1_out, d2_out)\n    true_graph = nx.DiGraph([(1, 2)])\n    true_graph.add_node(1, func=true_m1)\n    true_graph.add_node(2, func=true_m2)\n    true_mfnet = MFNetJax(true_graph)\n\n    x_train = jax.random.normal(data_key, (100, d_in))\n    y_train = true_mfnet.run((1, 2), x_train)\n\n    # 2. Create the MFNetJax model to be trained\n    train_m1 = LinearModel(init_linear_params(train_key, d_in, d1_out))\n    train_m2 = init_linear_scale_shift_model(train_key, d_in, d1_out, d2_out)\n    train_graph_struct = nx.DiGraph([(1, 2)])\n    train_graph_struct.add_node(1, func=train_m1)\n    train_graph_struct.add_node(2, func=train_m2)\n    mfnet_to_train = MFNetJax(train_graph_struct)\n\n    # 3. Set up the training loop with Optax\n    params, treedef = tree_util.tree_flatten(mfnet_to_train)\n    optimizer = optax.adam(learning_rate=1e-3)\n    opt_state = optimizer.init(params)\n\n    def loss_fn(p, x, y):\n        model = treedef.unflatten(p)\n        return mse_loss_graph(model, nodes=(1, 2), x=x, y=y)\n\n    @jax.jit\n    def step(p, opt_s, x, y):\n        loss_val, grads = jax.value_and_grad(loss_fn)(p, x, y)\n        updates, opt_s = optimizer.update(grads, opt_s)\n        p = optax.apply_updates(p, updates)\n        return p, opt_s, loss_val\n\n    print(\"--- Starting Training ---\")\n    initial_loss = loss_fn(params, x_train, y_train)\n    print(f\"Initial Loss: {initial_loss:.4f}\")\n\n    for i in range(2000):\n        params, opt_state, loss = step(params, opt_state, x_train, y_train)\n        if (i + 1) % 500 == 0:\n            print(f\"  Step {i+1}, Loss: {loss:.4f}\")\n\n    # 4. Make predictions with the final trained model\n    mfnet_fitted = treedef.unflatten(params)\n    x_test = jax.random.normal(data_key, (10, d_in))\n    predictions = mfnet_fitted.run((1, 2), x_test)\n\n    print(\"\\n--- Predictions from Node 2 (Highest Fidelity) ---\")\n    print(predictions[1])\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"}]}